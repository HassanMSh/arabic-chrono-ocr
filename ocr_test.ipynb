{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5fdee4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, re\n",
    "import datetime\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from kraken import binarization, blla, rpred\n",
    "from kraken.lib import models\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "import json\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fea09a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving images to images/run_20250831_224411\n",
      "[INFO] Saving OCR text to ocr/run_20250831_224411\n"
     ]
    }
   ],
   "source": [
    "# Create base dirs if they don't exist\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"ocr\", exist_ok=True)\n",
    "\n",
    "# Create timestamped run dirs\n",
    "timestamp = datetime.datetime.now().strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "img_run_dir = os.path.join(\"images\", timestamp)\n",
    "ocr_run_dir = os.path.join(\"ocr\", timestamp)\n",
    "os.makedirs(img_run_dir, exist_ok=True)\n",
    "os.makedirs(ocr_run_dir, exist_ok=True)\n",
    "\n",
    "print(f\"[INFO] Saving images to {img_run_dir}\")\n",
    "print(f\"[INFO] Saving OCR text to {ocr_run_dir}\")\n",
    "\n",
    "json_path = os.path.join(ocr_run_dir, \"ocr_output.json\")\n",
    "ocr_results = []\n",
    "\n",
    "model_path = \"models/arabic_best.mlmodel\"\n",
    "# Load OCR model\n",
    "model = models.load_any(\"models/arabic_best.mlmodel\")\n",
    "\n",
    "os.makedirs(\"gt\", exist_ok=True)\n",
    "\n",
    "# Convert page 11 from PDF to images\n",
    "pages = convert_from_path(\"books/attacks.pdf\", dpi=300, first_page=11, last_page=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8361be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def contour_crops(img_path, out_dir, prefix):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # binarize (Otsu, or replace with Sauvola)\n",
    "    _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "    # invert if text is white on black\n",
    "    if np.mean(thresh) > 127:\n",
    "        thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "    # find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    crops = []\n",
    "    for j, cnt in enumerate(contours):\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "        # skip tiny blobs\n",
    "        if w*h < 500:\n",
    "            continue\n",
    "\n",
    "        crop = img[y:y+h, x:x+w]\n",
    "        out_path = os.path.join(out_dir, f\"{prefix}_roi{j}.png\")\n",
    "        cv2.imwrite(out_path, crop)\n",
    "        crops.append(out_path)\n",
    "\n",
    "    return crops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1120aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 11\n",
    "\n",
    "for i, page in enumerate(pages, start=page):\n",
    "    w, h = page.size\n",
    "    halves = {\n",
    "        \"right\": page.crop((w // 2, 0, w, h)),\n",
    "        \"left\": page.crop((0, 0, w // 2, h)),\n",
    "    }\n",
    "\n",
    "    for side, img in halves.items():\n",
    "        # Convert PIL → OpenCV\n",
    "        cv_img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Binarize for contour detection (only for segmentation, not OCR input)\n",
    "        _, thresh = cv2.threshold(cv_img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        if np.mean(thresh) > 127:\n",
    "            thresh = cv2.bitwise_not(thresh)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Sort top-to-bottom, left-to-right\n",
    "        contours = sorted(contours, key=lambda c: (cv2.boundingRect(c)[1], cv2.boundingRect(c)[0]))\n",
    "\n",
    "        roi_texts = []\n",
    "        for j, cnt in enumerate(contours):\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "\n",
    "            # Skip small blobs\n",
    "            if w * h < 500:\n",
    "                continue\n",
    "\n",
    "            # Crop ROI and convert back to PIL for kraken\n",
    "            roi = img.crop((x, y, x + w, y + h))\n",
    "\n",
    "            # Directly segment + OCR (no binarization here)\n",
    "            seg = blla.segment(roi)\n",
    "            pred = rpred.rpred(model, roi, seg)\n",
    "            roi_texts.append(\"\\n\".join([line.prediction for line in pred]))\n",
    "\n",
    "        # Collect results for JSON\n",
    "        ocr_results.append({\n",
    "            \"page\": i,\n",
    "            \"side\": side,\n",
    "            \"text\": \"\\n\".join(roi_texts)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c5c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ANN models/arabic_best.mlmodel\t✓\n",
      "✓\n",
      "Binarizing\tBinarizing\t✓\n",
      "Segmenting images/run_20250829_164822/page_11_right.png\t✓\n",
      "Segmenting images/run_20250829_164822/page_11_right.png\t✓\n",
      "\u001b[2;36m[08/29/25 19:37:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Recognizers with segmentation types    \u001b]8;id=78997;file:///home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py\u001b\\\u001b[2mrpred.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=727590;file:///home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py#103\u001b\\\u001b[2m103\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'baselines'\u001b[0m\u001b[1m}\u001b[0m will be applied to       \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         segmentation of type bbox. This will   \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         likely result in severely degraded     \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         performace                             \u001b[2m            \u001b[0m\n",
      "\u001b[2KProcessing \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[35m0/32\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m✓\n",
      "\u001b[2;36m[08/29/25 19:37:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Recognizers with segmentation types    \u001b]8;id=78997;file:///home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py\u001b\\\u001b[2mrpred.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=727590;file:///home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py#103\u001b\\\u001b[2m103\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'baselines'\u001b[0m\u001b[1m}\u001b[0m will be applied to       \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         segmentation of type bbox. This will   \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         likely result in severely degraded     \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         performace                             \u001b[2m            \u001b[0m\n",
      "\u001b[2KProcessing \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[35m32/32\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:06\u001b[0m[0m \u001b[36m0:00:02\u001b[0m \u001b[33m0:00:06\u001b[0m\n",
      "\u001b[?25hWriting recognition results for images/run_20250829_164822/page_11_right.png\t✓\n",
      "\u001b[2KProcessing \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[35m32/32\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:06\u001b[0m\n",
      "\u001b[?25hWriting recognition results for images/run_20250829_164822/page_11_right.png\t✓\n",
      "[OK] gt/page_11_right.xml\n",
      "Extracted 6 entries ending with numbers/dates\n",
      "[OK] gt/page_11_right.xml\n",
      "Extracted 6 entries ending with numbers/dates\n",
      "Loading ANN models/arabic_best.mlmodel\tLoading ANN models/arabic_best.mlmodel\t✓\n",
      "✓\n",
      "Binarizing\tBinarizing\t✓\n",
      "Segmenting images/run_20250829_164822/page_11_left.png\t✓\n",
      "Segmenting images/run_20250829_164822/page_11_left.png\t✓\n",
      "\u001b[2;36m[08/29/25 19:37:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Recognizers with segmentation types    \u001b]8;id=606265;file:///home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py\u001b\\\u001b[2mrpred.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=392394;file:///home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py#103\u001b\\\u001b[2m103\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'baselines'\u001b[0m\u001b[1m}\u001b[0m will be applied to       \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         segmentation of type bbox. This will   \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         likely result in severely degraded     \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         performace                             \u001b[2m            \u001b[0m\n",
      "\u001b[2KProcessing \u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m  0%\u001b[0m \u001b[35m0/32\u001b[0m \u001b[36m-:--:--\u001b[0m \u001b[33m0:00:00\u001b[0m✓\n",
      "\u001b[2;36m[08/29/25 19:37:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[33mWARNING \u001b[0m Recognizers with segmentation types    \u001b]8;id=606265;file:///home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py\u001b\\\u001b[2mrpred.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=392394;file:///home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py#103\u001b\\\u001b[2m103\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         \u001b[1m{\u001b[0m\u001b[32m'baselines'\u001b[0m\u001b[1m}\u001b[0m will be applied to       \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         segmentation of type bbox. This will   \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         likely result in severely degraded     \u001b[2m            \u001b[0m\n",
      "\u001b[2;36m                    \u001b[0m         performace                             \u001b[2m            \u001b[0m\n",
      "\u001b[2KProcessing \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[35m32/32\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:07\u001b[0m6m0:00:02\u001b[0m \u001b[33m0:00:07\u001b[0m\n",
      "\u001b[?25hWriting recognition results for images/run_20250829_164822/page_11_left.png\t✓\n",
      "\u001b[2KProcessing \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[35m32/32\u001b[0m \u001b[36m0:00:00\u001b[0m \u001b[33m0:00:07\u001b[0m\n",
      "\u001b[?25hWriting recognition results for images/run_20250829_164822/page_11_left.png\t✓\n",
      "[OK] gt/page_11_left.xml\n",
      "Extracted 2 entries ending with numbers/dates\n",
      "[OK] gt/page_11_left.xml\n",
      "Extracted 2 entries ending with numbers/dates\n"
     ]
    }
   ],
   "source": [
    "page = 11\n",
    "\n",
    "for i, page in enumerate(pages, start=page):\n",
    "    w, h = page.size\n",
    "    halves = {\n",
    "        \"right\": page.crop((w // 2, 0, w, h)),\n",
    "        \"left\": page.crop((0, 0, w // 2, h)),\n",
    "    }\n",
    " \n",
    "\n",
    "    for side, img in halves.items():\n",
    "        img_path = os.path.join(img_run_dir, f\"page_{i}_{side}.png\")\n",
    "        xml_out = os.path.join(\"gt\", f\"page_{i}_{side}.xml\")\n",
    "        img.save(img_path)\n",
    "        # contour step\n",
    "        roi_dir = os.path.join(img_run_dir, f\"page_{i}_{side}_rois\")\n",
    "        roi_imgs = contour_crops(img_path, roi_dir, f\"page_{i}_{side}\")\n",
    "    \n",
    "        \n",
    "\n",
    "        cmd = [\"kraken\", \"-x\", \"-i\", img_path, xml_out, \"binarize\", \"segment\", \"ocr\", \"-m\", model_path]\n",
    "        \n",
    "        try:\n",
    "            subprocess.run(cmd, check=True)\n",
    "            print(f\"[OK] {xml_out}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"[ERR] {img_path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
