{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fdee4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, re\n",
    "import datetime\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from kraken import binarization, blla, rpred\n",
    "from kraken.lib import models\n",
    "from dateutil import parser\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea09a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Saving images to images/run_20250828_162751\n",
      "[INFO] Saving OCR text to ocr/run_20250828_162751\n"
     ]
    }
   ],
   "source": [
    "# Create base dirs if they don't exist\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "os.makedirs(\"ocr\", exist_ok=True)\n",
    "\n",
    "# Create timestamped run dirs\n",
    "timestamp = datetime.datetime.now().strftime(\"run_%Y%m%d_%H%M%S\")\n",
    "img_run_dir = os.path.join(\"images\", timestamp)\n",
    "ocr_run_dir = os.path.join(\"ocr\", timestamp)\n",
    "os.makedirs(img_run_dir, exist_ok=True)\n",
    "os.makedirs(ocr_run_dir, exist_ok=True)\n",
    "\n",
    "print(f\"[INFO] Saving images to {img_run_dir}\")\n",
    "print(f\"[INFO] Saving OCR text to {ocr_run_dir}\")\n",
    "\n",
    "# CSV output file\n",
    "csv_path = os.path.join(ocr_run_dir, \"ocr_output.csv\")\n",
    "\n",
    "# Load OCR model\n",
    "model = models.load_any(\"models/arabic_best.mlmodel\")\n",
    "\n",
    "# Convert page 11 from PDF to images\n",
    "pages = convert_from_path(\"books/attacks.pdf\", dpi=300, first_page=11, last_page=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ec31954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py:322: UserWarning: Using legacy polygon extractor, as the model was not trained with the new method. Please retrain your model to get speed improvement.\n",
      "  warnings.warn('Using legacy polygon extractor, as the model was not trained with the new method. Please retrain your model to get speed improvement.')\n",
      "/home/hassan/projects/arabic-chrono-ocr/venv/lib/python3.12/site-packages/kraken/rpred.py:322: UserWarning: Using legacy polygon extractor, as the model was not trained with the new method. Please retrain your model to get speed improvement.\n",
      "  warnings.warn('Using legacy polygon extractor, as the model was not trained with the new method. Please retrain your model to get speed improvement.')\n"
     ]
    }
   ],
   "source": [
    "# Convert Arabic-Indic digits → ASCII digits\n",
    "def normalize_digits(s: str) -> str:\n",
    "    trans = str.maketrans(\"٠١٢٣٤٥٦٧٨٩\", \"0123456789\")\n",
    "    return s.translate(trans)\n",
    "\n",
    "# Extract dates from text\n",
    "def extract_dates(text: str):\n",
    "    text_norm = normalize_digits(text)\n",
    "    date_pattern = re.compile(r'(\\d{2,4})[/-](\\d{1,3})[/-](\\d{1,4})')  # allow OCR glitches\n",
    "    matches = date_pattern.findall(text_norm)\n",
    "\n",
    "    raw_dates = []\n",
    "    clean_dates = []\n",
    "\n",
    "    for y, m, d in matches:\n",
    "        raw_dates.append(f\"{y}/{m}/{d}\")  # save raw fragment\n",
    "\n",
    "        try:\n",
    "            y = int(y)\n",
    "            m = int(m)\n",
    "            d = int(d)\n",
    "\n",
    "            # Fix 2-digit years (assume 1900s for this corpus)\n",
    "            if y < 100:\n",
    "                y = 1900 + y\n",
    "\n",
    "            # Validate ranges\n",
    "            if not (1 <= m <= 12):\n",
    "                continue\n",
    "            if not (1 <= d <= 31):\n",
    "                continue\n",
    "\n",
    "            clean_dates.append(f\"{y:04d}-{m:02d}-{d:02d}\")\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Return semicolon-separated\n",
    "    return \";\".join(raw_dates), \";\".join(sorted(set(clean_dates)))\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"page\", \"side\", \"text\", \"dates_raw\", \"dates_normalized\"])  # header\n",
    "\n",
    "    for i, page in enumerate(pages, start=11):\n",
    "        w, h = page.size\n",
    "        halves = {\n",
    "            \"right\": page.crop((w // 2, 0, w, h)),  # RTL order: right first\n",
    "            \"left\": page.crop((0, 0, w // 2, h)),\n",
    "        }\n",
    "\n",
    "        for side, img in halves.items():\n",
    "            img_path = os.path.join(img_run_dir, f\"page_{i}_{side}.png\")\n",
    "            img.save(img_path)\n",
    "\n",
    "            # OCR\n",
    "            bin_img = binarization.nlbin(img)\n",
    "            seg = blla.segment(bin_img)\n",
    "            pred = rpred.rpred(model, bin_img, seg)\n",
    "            text = \"\\n\".join([line.prediction for line in pred])\n",
    "\n",
    "            # Dates\n",
    "            dates_raw, dates_norm = extract_dates(text)\n",
    "\n",
    "            # Write to CSV\n",
    "            writer.writerow([i, side, text, dates_raw, dates_norm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76560fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d476df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode normalized_date into rows\n",
    "df = df.assign(normalized_date=df['dates_normalized'].str.split(';')).explode('normalized_date')\n",
    "df = df[df['normalized_date'].notna() & (df['normalized_date'] != \"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd5f6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_or_nat(date_str):\n",
    "    try:\n",
    "        y, m, d = map(int, date_str.split(\"-\"))\n",
    "\n",
    "        # year must be 1900–1960 (adjust window for your data)\n",
    "        if not (1900 <= y <= 1960):\n",
    "            return pd.NaT\n",
    "\n",
    "        # month/day sanity\n",
    "        if not (1 <= m <= 12):\n",
    "            return pd.NaT\n",
    "        if not (1 <= d <= 31):\n",
    "            return pd.NaT\n",
    "\n",
    "        return pd.to_datetime(f\"{y:04d}-{m:02d}-{d:02d}\", errors=\"coerce\")\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "\n",
    "df['dt'] = df['normalized_date'].apply(validate_or_nat)\n",
    "\n",
    "# anchors\n",
    "df['prev_valid'] = df['dt'].ffill()\n",
    "df['next_valid'] = df['dt'].bfill()\n",
    "\n",
    "def contextual_fix(row):\n",
    "    if pd.notna(row['dt']):\n",
    "        return row['dt']  # already valid\n",
    "    # fallback to prev/next\n",
    "    if pd.notna(row['prev_valid']) and pd.notna(row['next_valid']):\n",
    "        # pick whichever is closer in days\n",
    "        prev_gap = abs((row['prev_valid'] - row['next_valid']).days)\n",
    "        return row['prev_valid'] if prev_gap <= 15 else row['next_valid']\n",
    "    if pd.notna(row['prev_valid']):\n",
    "        return row['prev_valid']\n",
    "    if pd.notna(row['next_valid']):\n",
    "        return row['next_valid']\n",
    "    return pd.NaT\n",
    "\n",
    "df['fixed_date'] = df.apply(contextual_fix, axis=1)\n",
    "df['fixed_date'] = df['fixed_date'].dt.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f173a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   page   side    raw_norm normalized_date  fixed_date\n",
      "0    11  right  1949-08-01      1949-08-01  1949-08-01\n",
      "0    11  right  1949-10-01      1949-10-01  1949-10-01\n",
      "0    11  right  1999-10-01      1999-10-01  1949-08-01\n",
      "0    11  right  1949-08-01      1949-08-01  1949-08-01\n",
      "0    11  right  1949-10-01      1949-10-01  1949-10-01\n",
      "0    11  right  1999-10-01      1999-10-01  1949-08-01\n",
      "0    11  right  1949-08-01      1949-08-01  1949-08-01\n",
      "0    11  right  1949-10-01      1949-10-01  1949-10-01\n",
      "0    11  right  1999-10-01      1999-10-01  1949-11-20\n",
      "1    11   left  1949-11-20      1949-11-20  1949-11-20\n"
     ]
    }
   ],
   "source": [
    "print(df[['page','side','raw_norm','normalized_date','fixed_date']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebdf911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   page   side    raw_norm  fixed_date\n",
      "0    11  right  1999-10-01  1949-08-01\n",
      "0    11  right  1999-10-01  1949-08-01\n",
      "0    11  right  1999-10-01  1949-11-20\n"
     ]
    }
   ],
   "source": [
    "audit = df[df['raw_norm'] != df['fixed_date']]\n",
    "print(audit[['page','side','raw_norm','fixed_date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06885c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reading_order(df):\n",
    "    # Ensure sort: page asc, then side (right before left)\n",
    "    df['side_order'] = df['side'].map({'right': 0, 'left': 1})\n",
    "    return df.sort_values(['page', 'side_order']).reset_index(drop=True)\n",
    "\n",
    "def split_across(df):\n",
    "    results = []\n",
    "    full_text = \"\"\n",
    "    meta = []\n",
    "\n",
    "    # Build continuous reading stream\n",
    "    for _, row in df.iterrows():\n",
    "        full_text += f\"\\n{row['text']}\"\n",
    "        meta.append((row['page'], row['side']))\n",
    "\n",
    "    # Regex split into blocks\n",
    "    matches = list(re.finditer(r'(\\d{2,4}[/-]\\d{1,2}[/-]\\d{1,4})', normalize_digits(full_text)))\n",
    "\n",
    "    for i, m in enumerate(matches):\n",
    "        raw_date = m.group(1)\n",
    "        start = m.end()\n",
    "        end = matches[i+1].start() if i+1 < len(matches) else len(full_text)\n",
    "        block_text = full_text[start:end].strip()\n",
    "\n",
    "        results.append({\n",
    "            \"raw_date\": raw_date,\n",
    "            \"text_block\": block_text\n",
    "            # optionally: earliest (page, side) from meta in this span\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43294f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV output file\n",
    "clean_csv_path = os.path.join(ocr_run_dir, \"ocr_output_clean.csv\")\n",
    "\n",
    "# Save cleaned dataset\n",
    "df.to_csv(clean_csv_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
